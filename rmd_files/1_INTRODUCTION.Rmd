---
output:
  html_document: default
  pdf_document: default
---
Many archaeologists are familiar with Bayesian statistics in the context of radiocarbon date calibration and chronology building. However, the Bayesian framework has broader applications beyond dating and chronology that are worthy of consideration by archaeologists. For example, many researchers in the natural and social sciences are using Bayesian statistics to evaluate how well observational or experimental data align with their hypotheses. For the most part, this use of Bayesian inference has not been applied to archaeology. Using a fictional zooarchaeological example, this paper provides a straightforward explanation of Bayesian inference and compares it to the more conventional null hypothesis significance testing (NHST). Although some have previously described and reviewed the application of these concepts elsewhere [@buck_bayesian_1996;@buck_applications_2001;@buck_being_2015;@otarola-castillo_bayesian_2018;@otarola-castillo_bayesian_2022;@wolfhagen_rethinking_2019;@wolfhagen_re-examining_2020], this work is focused on presenting replicable step-by-step examples of the Bayesian framework for evaluating and discerning among competing hypotheses. In addition, a Spanish translation of this manuscript and associated materials is also available in an Open Science Framework repository: https://osf.io/23bdt/.


### Uncertainty and probability in archaeological applications

All data are uncertain. Measurements and observations are not exact, and their resulting values are variably imprecise. Archaeologists routinely use statistical quantities such as variance, standard deviation, and standard error, which rely on probability theory to describe this uncertainty. In their field and lab work, archaeologists regularly use equipment that relies on probabilistic descriptions of uncertainty. For example, the manufacturer of total stations, widely used to map archaeological sites, has stated accuracies of 2 mm plus an additional 2 mm per km, usually at the 1 sigma standard deviation level (e.g., Leica TS16). This is an example of a probability concept used to measure “random” uncertainty. In this case, assuming a “normal” probability distribution for the measurement error (although the manufacturer does not specify this), archaeologists should expect that 68% of the location of artifacts mapped by this instrument will have an error up to ± 2 mm, plus error related to increasing distance (and error due to atmospheric conditions, instrument stability, etc.  [@walker_total_2020]). Similarly, the manufacturer’s specification sheet for a typical Ohaus (Scout STX2202) portable digital scale claims to measure up to 2,200 grams, with an error of ± 0.02 g (1 sigma). Like the total stations, if we assume a normal error model, this means that the manufacturer certifies that 68% of all readings are within ±0.02 grams of the true reading under ideal circumstances. 
 Similarly, after careful data collection and analyses, archaeologists also apply the concept of probability to test their hypotheses. These are formal statements that offer plausible explanations of the observed patterns of people or their environment in the past. Like the statements about field and laboratory instrument measurements, these hypotheses and their predictions also possess some degree of uncertainty due to incomplete observation or knowledge. To formally quantify uncertainty about data and hypotheses, archaeologists frequently rely on specific probability models or probability functions (i.e., equations). The inputs of a probability function are observed or hypothesized values, and the outcomes are their probabilities ranging from zero to one, i.e., from least to most plausible. Archaeologists use this probabilistic system to test their hypotheses and describe the degree of uncertainty with which their hypotheses account for their current and likely future observations. Using a probabilistic approach gives archaeologists a powerful and systematic tool that makes it possible to interpret data and evaluate hypotheses. 
Below, we provide an overview of the central concepts of the two major probability paradigms to evaluate hypotheses: NHST and Bayesian inference. Whereas most scientists widely use NHST, the Bayesian approach is considered a modern data-driven learning system that has enjoyed increasing application in archaeology [@jaynes_probability_2003;@howson_scientific_2006;@buck_bayesian_1996,@buck_being_2015;@otarola-castillo_differentiating_2018;@otarola-castillo_bayesian_2022]

### Null hypothesis significance testing

As the prevailing statistical framework in most sciences, NHST enables practitioners to use their data to evaluate hypotheses. This approach is rooted in the early 20th-century development of goodness of fit tests [@fisher_interpretation_1922;@pearson_x_1900], experimental design, p-values [@fisher_statistical_1925;@fisher_design_1935], confidence intervals (CIs) and hypothesis testing [@neyman_problem_1933,p.294]. This methodology was introduced to archaeology in the mid-20th century [@binford_consideration_1964;@clarke_analytical_1968;@myers_applications_1950;@spaulding_statistical_1953;@vescelius_archaeological_1960]. Applications of NHST in archaeology continue today, supported by new archaeology-specific statistical textbooks [@banning_archaeologists_2020;@baxter_statistics_2003;@carlson_quantitative_2017;@drennan_statistics_2010;@fletcher_digging_2005;@mccall_strategies_2018;@shennan_quantifying_1997].These textbooks provide detailed treatment of NHST and its procedures in the context of archaeology (for a multidisciplinary introductory textbook to NHST see for example [@diez_openintro_2019]).

In general, however, the NHST paradigm revolves around the concept of theoretically repeated sampling over the long term and the Central Limit Theorem (CLT; @diez_openintro_2019[p.172]). The CLT informs NHST's approach to hypothesis description and evaluation. The theorem shows that given a large enough sample, in many cases, summary statistics (e.g., mean or standard deviation) will follow a normal distribution. For instance, after sampling the same population multiple times, the means of individual samples will be normally distributed. This phenomenon occurs often, even if the original variable was not normally distributed, this concept applies to many situations and data. The CLT further links sample statistics to their null distributions, such as the mean, through its standard error. According to the CLT, the standard error of a sample's mean estimates the standard deviation of the mean's null distribution. One may compute this quantity by dividing the sample's standard deviation by the sample size.

The CLT is helpful to archaeologists who often sample from a target population—a group of individuals, artifacts, events, measurements, or other phenomena that they wish to study. The aim is to use the sample to test a priori hypotheses about quantifiable characteristics of the sampled population. Statisticians refer to these characteristics as the population parameters. For example, a population's mean and standard deviation parameters represent its central tendency and variability, respectively. Sample statistics function as estimates of the population parameters and are thus also known as the parameter estimates. These statistics are used to test hypotheses about their respective population parameters. NHST requires archaeologists to state only two hypotheses: a null and an alternative hypothesis to evaluate. Null hypotheses are quantitative statements of "no difference" (difference = 0) between a hypothesized parameter value and its sample statistic, or between a sample statistic and its counterpart from another sample. Archaeologists often set up such null hypotheses to evaluate whether a sample statistic resulted from a population having the hypothesized parameter value (i.e., a one-sample test).Alternatively, they may wish to know if the statistics from two independent samples were drawn from the same population (i.e., a two-sample test).
Alternative hypotheses are ordinarily simple statements negating the null hypothesis. Once archaeologists state the null and alternative hypotheses, they then sample the population, or “collect data,” and calculate the sample statistics. We should point out that the NHST framework proceeds by assuming that the null  hypothesis is true and then using the sample data, summarized by a statistic, to test that assumption. To do so, archaeologists use the sample statistic to define a test statistic (frequently the z-, t-, F-ratios, and chi-square values; e.g., @diez_openintro_2019;@thomas_reconfiguring_1986; @drennan_statistics_2010[p.177]) and calculate the probability that a value equal to or more extreme than the test statistic can occur under the assumption of the null hypothesis.
The probability of the test statistic, or p-value, is often calculated with the help of probability distribution models, like the normal distribution. These probability models are also known as likelihood functions. The likelihood is a statistical function that describes the probability of the test statistic dependent on the hypothesized parameter values, e.g., those assumed by the null hypothesis. For instance, as we show in the fictitious example below, the normal likelihood function is used to compute the p-value of a z-ratio test statistic, assuming the null hypothesis is true. Using similar probability models, archaeologists conduct NHST and calculate quantities such as p-values and confidence intervals (CIs) to evaluate whether the test statistic rejects or fails to reject the null hypothesis.
Confidence intervals (CIs) are grounded on the CLT's null distribution concept. Archaeologists often compute CIs in two contexts: 1) to conduct NHST, they calculate the CIs of a test statistic, and 2) to estimate the precision of a parameter estimate, they compute the CIs of a sample statistic. Generally, the CIs of either the test or sample statistic, are centered on their mean, represent their respective null distribution and are derived using their sample's standard error. Recall that the standard error of the either statistic is the standard deviation of its null distribution. For the sample statistic, this distribution represents the range of plausible values within which one may find the true value of the population parameters.
In the context of the test statistic, however, the CI is the range of possible within which the true difference, assumed by the null hypothesis, will be found.In other words, due to the CLT, ~68% of the test statistic's null distribution will capture the true value of the difference assumed to be zero by the null hypothesis. Likewise, in the case of a sample statistic, 68% of its null distribution will contain the true value of the population parameter. Alternatively, one may wish less uncertainty than 68% for the sample or test statistic. In this case, one may compute ranges similar to the standard error that capture the true parameter or difference values 95%-to-99% of the time-again, after theoretically repeated sampling. These ranges are the CIs, and we refer to them in terms of their percentage: e.g., as 95% or 99% CIs. In the context of NHST, archaeologists use CIs to reject or fail to reject a null hypothesis. If the value of no difference, 0, is within the test statistic's CI, then the null hypothesis fails to be rejected. However, if 0 is not within the test statistic's CI range, the null hypothesis is not supported by the data and is rejected in favor of the alternative. We offer one last note about the mechanics of CIs. It may seem tempting to interpret 95% CI as indicating that the true population parameter or difference has a 0.95 probability of being in the CI. Although somewhat confusing, however, the correct interpretation of the CI is that, based on repeated sampling over the long term, 95% of the CIs will contain the true population parameter or difference. 

In addition to CIs, NHST uses p-values as an empirical signal of the plausibility of the test statistic assuming the null hypothesis is true. Archaeologists compute p-values by calculating the proportion of values in the null distribution that are equal to and more extreme than the sample’s test statistic. Typically, test statistic values with a p-value less than or equal to a proportion of 0.05 (1 out 20 or 5%) are considered extreme. Archaeologists commonly judge whether to reject or fail to reject the null hypothesis using a p-value of 0.05 as a cut-off for rejection: the more extreme the data, the smaller the p-value.

The broader scientific community has become increasingly critical of NHST [@gelman_failure_2018;@gelman_multilevel_2006;@vidgen_p-values_2016]. Statisticians have strongly pointed out the arbitrariness of the 0.05 p-value threshold for statistical significance [@cowgill_trouble_1977;@valeggia_moving_2022; @Wasserstein_movingbeyond_2019]. Some argue that inadequate statistical training may lead researchers to misunderstand p-values, [@hubbard_widespread_2011;@mcshane_blinding_2015]. One consequence of not fully understanding the concept of p-values, for instance, is that some researchers confuse practical significance, or relevance, for statistical significance. In particular, it is possible for effects that are practically negligible, irrelevant or uninteresting to result in small p-values [@aarts_insignificance_2012;@johnson_insignificance_1999;@kramer_sibling_2016;@mccall_strategies_2018;@wolverton_practical_2016]. In one case, while investigating the effects of sibling competition on the growth patterns of Maya children, Kramer et al. [-@kramer_sibling_2016] found that the effects of family size on child growth were statistically significant but “of little consequence to early childhood health or fitness.” Here, interpreting the 0.05 p-value cutoff would have led to incorrect conclusions. 
In other cases, some researchers have confused p-values for the type-I error rate, α. The p-value is the probability that the test statistic may occur under the null hypothesis; α is the probability of rejecting the null hypothesis when it is true [@hubbard_widespread_2011]. Historically, these two statistical quantities belong to competing NHST philosophies [@fisher_statistical_1925;@neyman_problem_1933]. Neyman and Pearson developed the concept of Type-1 error in the context of designing infinitely repeatable experiments, wherein α defines the probability that an analysis will fail to find a difference between two hypotheses when there is a genuine difference. Fisher's p-value, by contrast, empirically estimates if a specific set of observations fit a specified null hypothesis. These two quantities have completely different theoretical underpinnings and relationships to actual observations. For example, α is unrelated to observations, and the p-value is not influenced by the alternative hypotheses under consideration. Typical NHST practice, unfortunately, can lead researchers to directly associate the two concepts, complicating efforts to provide reasonable definitions and interpretations [@hubbard_confusion_2003]. The misuse of p-values and statistical significance, due to either misunderstanding [@thiese_misuse_2015] or intention [@chuard_evidence_2019;@head_extent_2015], can lead to the so-called scientific replication crisis [@ioannidis_why_2005], which is beginning to reach archaeological science [@bayliss_confessions_2019;@marwick_computational_2017;@mcpherron_machine_2021-3] 

Even accounting for these nuances, the interpretation of NHST concepts such as p-values, statistical significance, hypothesis testing, and CIs is not entirely straightforward. Statements about sample statistics—standard errors and CIs—are based on hypothetical repeated sampling, which is difficult to conceive of in non-experimental situations or, as in archaeology, where true replication is hard or even impossible to achieve. In terms of evaluation, although most researchers might generally understand how to interpret a significant p-value in the context of rejecting a null hypothesis, the meaning of a non-significant p-value may cause confusion. This confusion might be exacerbated by the fact that NHST has no mechanism for “accepting” or “verifying” a null hypothesis. This critical misunderstanding of NHST may lead some to interpret a non-significant p-value as acceptance of their null hypothesis rather than “failing to reject it" [@greenland_statistical_2016]. However, knowledge production in the NHST paradigm is centered on rejecting null hypotheses, rather than accepting the null or alternative hypotheses. To be fair, the NHST language is confusing. For example, stating that a null hypothesis failed to be rejected is a triple negative, meaning that "the hypothesis of no difference was not not-accepted." Such convoluted language embedded in NHST obfuscates the relationship between the p-value, null and alternative hypotheses.
Moreover, the role of the alternative hypothesis and its connection to the p-value are also unclear and often incorrectly interpreted [@cohen_earth_1994;@benjamin_three_2019]. As a result, inference using traditional NHST statistics can be difficult, especially when a study is wishes to discern among multiple working hypotheses [@chamberlin_method_1965;@gelman_why_2012], for example, when two or more hypotheses fail to be rejected. In theory, such hypotheses are consistent with the data. However, ranking multiple unrejected null hypotheses is difficult, if not impossible. One way to rank them may be to use the hypotheses' p-values. After all, the p-value is a continuous metric mediating hypothesis rejection and failure-to-reject. However, statisticians discourage this procedure [@hubbard_why_2008;@mcshane_abandon_2019] because the magnitude of the p-value does not reflect the weight of evidence of one hypothesis over another. Consequently, traditional NHST does not offer a straightforward procedure for further comparing “unrejected” null hypotheses. 

### Bayesian statistics

Bayesian inference offers an alternative approach with several advantages over NHST. First, Bayesian statistics enables scientists to use data to assign probabilities to their parameter estimates and hypotheses, facilitating a more straightforward comparison of competing hypotheses. Second, while NHST uses only new data to inferences, a Bayesian framework allows both new data and existing information to be combined. As we detail below, this characteristic more closely resembles scientists' decision making processes and is likely one of the key reasons scientists, including anthropologists and archaeologists, are increasingly adopting the Bayesian inference to evaluate their hypotheses. 

Bayes’ theorem derives its name from the Reverend Thomas
 [-@bayes_essay_1763], an English Presbyterian minister and mathematician who researched problems in probability that involved conditional and prior probabilities (defined below). However, it was not until the late 1900s that the Bayesian approach to statistical inference was popularized in science [@bellhouse_reverend_2004].Although archaeologists notably began adopting Bayesian statistics to assess hypotheses in the 1990s [@buck_bayesian_1996;@cowgill_distinguished_1993], earlier applications can be found scattered throughout the archaeological literature beginning in the 1970s [@doran_mathematics_1975;@fisher_mastodont_1987;@freeman_bayesian_1976;@thomas_reconfiguring_1986;@salmon_philosophy_1982]. Today, scientists, including anthropologists and archaeologists who find this approach advantageous, are increasingly applying Bayesian statistics to evaluate their hypotheses with data [@gelman_bayesian_2020;@mcelreath_statistical_2020;@naylor_archaeological_1988;@otarola-castillo_bayesian_2018]. 

One advantage of Bayesian inference is that it enables expert, or prior, information about hypotheses to be incorporated into a statistical analysis. As we show in our example below, the prior knowledge of an archaeologist or collection of archaeologists and other experts can be very valuable, especially in archaeology, as “we depend very much on prior information to help us in evaluating the degree of plausibility in a new problem” [@jaynes_probability_2003,p.6]. Formally, including previous experience or expert information into statistical analyses to “update” one’s state of knowledge is a natural learning process and improves the inferences made by NHST [@cowgill_past_2001]. To accomplish this, practitioners of Bayesian inference convert prior knowledge into prior probabilities and use them and their distributions as part of statistical analyses. Once analysts determine their prior probability distributions, as with NHSt, they can observe new data to test their hypothesis (or hypotheses). In this context, the liklihood of the data is combined with (or weighted by) the prior to give Bayesian posterior probability. The posterior is the probability of the hypothesis given the observed data's likelihood and prior knowledge [@buck_bayesian_1996]. As we discuss in more detail below, the Bayesian process is particularly helpful in situations where only small amounts of data are obtained, as is often the case in archaeology.

In simple cases, determining the posterior and its distribution is relatively straightforward. However, the calculus underlying more complex cases is impossible to solve without the application of novel simulation methods. In particular, the Markov Chain Monte Carlo (MCMC) algorithms has facilitated progress of Bayesian analyses. MCMC is a combination of Monte Carlo sampling and Markov Chains. Monte Carlo sampling is used to estimate difficult to compute quantities from the unknown distribution of an observed random variable. Markov Chains are a stochastic series of events associated with one another, where the probability of a new event is dependent only on the state of the last event. Together, these characteristics of Monte Carlo sampling and Markov Chains are essential to find the posterior probability distribution of complex problems. Today, variations on the original MCMC algorithms [@metropolis_equation_1953], such as the Metropolis-Hastings, Gibbs, and Hamiltonian , and other methods, are now in widespread use, facilitating broad application of the Bayesian paradigm [@dunson_hastings_2020;@gilks_markov_1995;@howson_scientific_2006;@robert_short_2011]. 

To further contextualize the application of Bayesian statistics, we provide a fictional example that illustrates how one can use this probabilistic framework to solve an idealized archaeological research problem. To do this, we choose to use a parable^[This example was inspired by the creative works similar to Neil Thompson’s (1972) The Mysterious Fall of  the Nacirema, Kent Flannery’s The Early Mesoamerican Village (1976) and The Golden Marshalltown (1982), and John Shea's Uwasi Valley Tales from "Prehistoric Stone Tools of Eastern Africa: A Guide" (2020).] rather than a real case study in order to avoid the complexities of site formation processes and sampling bias. The contrived, fictional example in this parable also helps focus attention on specific aspects of Bayesian inference, which we feel are most instructive. The parable of the “Monico Culture and the Bayesian Archaeologist” demonstrates how inferences can be made using data and prior information about a hypothesis, how to evaluate the uncertainty surrounding a hypothesis, why this approach seems less ambiguous than NHST, and thus, why it is becoming increasingly popular.